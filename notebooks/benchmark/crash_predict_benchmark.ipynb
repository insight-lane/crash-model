{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model for crash prediction\n",
    "### Developed by: bpben\n",
    "#### Details steps of data processing, feature engineering and model tuning/testing for crash and road data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "from glob import glob\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for tuning/testing models, available [here](https://github.com/bpben/model_helpers) as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.ensemble as ske\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn import metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn import model_selection as cv\n",
    "\n",
    "class Indata():\n",
    "    scoring = None\n",
    "    data = None\n",
    "    train_x, train_y, test_x, test_y = None, None, None, None\n",
    "    is_split = 0\n",
    "    \n",
    "    #init with pandas DF and target column name, specify scoring observations\n",
    "    def __init__(self, data, target, scoring=None):\n",
    "        #If scoring observations, store under scoring attribute\n",
    "        if scoring is not None:\n",
    "            self.data = data[~(scoring)]\n",
    "            self.scoring = data[scoring]\n",
    "        else:\n",
    "            self.data = data\n",
    "        self.target = target\n",
    "    \n",
    "    # Split into train/test\n",
    "    # pct = percent training observations\n",
    "    # datesort = specify date column for sorting values\n",
    "    #   If this is not None, split will be non-random (i.e. split on sorted obs)\n",
    "    def tr_te_split(self, pct, datesort=None):\n",
    "        if datesort:\n",
    "            self.data.sort_values(datesort, inplace=True)\n",
    "            self.data.reset_index(drop=True, inplace=True)\n",
    "            inds = np.arange(0.0,len(self.data)) / len(self.data) < pct\n",
    "        else:\n",
    "            inds = np.random.rand(len(self.data)) < pct\n",
    "        self.train_x = self.data[inds]\n",
    "        print 'Train obs:', len(self.train_x)\n",
    "        self.train_y = self.data[self.target][inds]\n",
    "        self.test_x = self.data[~inds]\n",
    "        print 'Test obs:', len(self.test_x)\n",
    "        self.test_y = self.data[self.target][~inds]\n",
    "        self.is_split = 1\n",
    "        \n",
    "class Tuner():\n",
    "    \"\"\"\n",
    "    Initiates with indata class, will tune series of models according to parameters.  \n",
    "    Outputs RandomizedGridCV results and parameterized model in dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    data = None\n",
    "    train_x, train_y = None, None\n",
    "    \n",
    "    def __init__(self, indata, best_models=None, grid_results=None):\n",
    "        if indata.is_split == 0:\n",
    "            raise ValueError('Data is not split, cannot be tested')\n",
    "        else:\n",
    "            self.data = indata.data\n",
    "            self.train_x = indata.train_x\n",
    "            self.train_y = indata.train_y\n",
    "            if best_models is None:\n",
    "                self.best_models = {}\n",
    "            if grid_results is None:\n",
    "                self.grid_results = pd.DataFrame()\n",
    "        \n",
    "            \n",
    "    def make_grid(self, model, obs, cvparams, mparams):\n",
    "        #Makes CV grid\n",
    "        grid = RandomizedSearchCV(\n",
    "                    model(),scoring=cvparams['pmetric'], \n",
    "                    cv = cv.KFold(cvparams['folds']), \n",
    "                    refit=False, n_iter=cvparams['iter'],\n",
    "                    param_distributions=mparams, verbose=1)\n",
    "        return(grid)\n",
    "    \n",
    "    def run_grid(self, grid, train_x, train_y):\n",
    "        grid.fit(train_x, train_y)\n",
    "        results = pd.DataFrame(grid.cv_results_)[['mean_test_score','mean_train_score','params']]\n",
    "        best = {}\n",
    "        best['bp'] = grid.best_params_\n",
    "        best[grid.scoring] = grid.best_score_\n",
    "        return(best, results)\n",
    "            \n",
    "    def tune(self, name, m_name, features, cvparams, mparams):\n",
    "        if hasattr(ske, m_name):\n",
    "            model = getattr(ske, m_name)\n",
    "        elif hasattr(skl, m_name):\n",
    "            model = getattr(skl, m_name)\n",
    "        else:\n",
    "            raise ValueError('Model name is invalid.')\n",
    "        grid = self.make_grid(model, len(self.train_x), cvparams, mparams)\n",
    "        best, results = self.run_grid(grid, self.train_x[features], self.train_y)\n",
    "        results['name'] = name\n",
    "        results['m_name'] = m_name\n",
    "        self.grid_results = self.grid_results.append(results)\n",
    "        best['model'] = model(**best['bp'])\n",
    "        best['features'] = list(features)\n",
    "        self.best_models.update({name: best}) \n",
    "        \n",
    "class Tester():\n",
    "    \"\"\"\n",
    "    Initiates with indata class, receives parameterized sklearn models, prints and stores results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, rundict=None):\n",
    "        if data.is_split == 0 :\n",
    "            raise ValueError('Data is not split, cannot be tested')\n",
    "        else:\n",
    "            self.data = data\n",
    "            if rundict is None:\n",
    "                self.rundict = {}\n",
    "            \n",
    "    #Add tuner object, will populate rundict with names, models, feature\n",
    "    def init_tuned(self, tuned):\n",
    "        if tuned.best_models=={}:\n",
    "            raise ValueError('No tuned models found')\n",
    "        else:\n",
    "            self.rundict.update(tuned.best_models)\n",
    "    \n",
    "    #Produce predicted class and probabilities\n",
    "    def predsprobs(self, model, test_x):\n",
    "        preds = model.predict(test_x)\n",
    "        probs = model.predict_proba(test_x)[:,1]\n",
    "        return(preds, probs)\n",
    "    \n",
    "    #Produce metrics\n",
    "    def get_metrics(self, preds, probs, test_y):\n",
    "        f1_s = metrics.f1_score(test_y, preds)\n",
    "        brier = metrics.brier_score_loss(test_y, probs)\n",
    "        auc = metrics.roc_auc_score(test_y, preds)\n",
    "        c_matrix = metrics.confusion_matrix(test_y, preds)\n",
    "        return(f1_s, brier, auc, c_matrix)\n",
    "    \n",
    "    #Run production, output dictionary\n",
    "    def make_result(self, model, test_x, test_y):\n",
    "        preds, probs = self.predsprobs(model, test_x)\n",
    "        f1_s, brier, auc, c_matrix = self.get_metrics(preds, probs, test_y)\n",
    "        print \"f1_score: \", f1_s\n",
    "        print \"brier_score: \", brier\n",
    "        print \"AUC score: \", auc\n",
    "        print \"Confusion Matrix: \", c_matrix\n",
    "        result = {}\n",
    "        #result['preds'] = [int(i) for i in preds]\n",
    "        #result['probs'] = [float(i) for i in probs]\n",
    "        result['f1_s'] = f1_s\n",
    "        result['brier'] = brier\n",
    "        result['auc'] = auc\n",
    "        result['c_matrix'] = c_matrix\n",
    "        return(result)\n",
    "\n",
    "    # Run model - Specify model, with parameters, features\n",
    "    # Stores it to rundict, can later be output\n",
    "    # Will overwrite previous run if name is not different\n",
    "    def run_model(self, name, model, features, cal=True, cal_m='sigmoid'):\n",
    "        results = {}\n",
    "        results['features'] = list(features)\n",
    "        print \"Fitting {} model with {} features\".format(name, len(features))\n",
    "        if cal:\n",
    "            # Need disjoint calibration/training datasets\n",
    "            # Split 50/50\n",
    "            rnd_ind = np.random.rand(len(self.data.train_x)) < .5\n",
    "            train_x = self.data.train_x[features][rnd_ind]\n",
    "            train_y = self.data.train_y[rnd_ind]\n",
    "            cal_x = self.data.train_x[features][~rnd_ind]\n",
    "            cal_y = self.data.train_y[~rnd_ind]\n",
    "        else:\n",
    "            train_x = self.data.train_x[features]\n",
    "            train_y = self.data.train_y\n",
    "\n",
    "        m_fit = model.fit(train_x, train_y)\n",
    "        result = self.make_result(\n",
    "            m_fit,\n",
    "            self.data.test_x[features],\n",
    "            self.data.test_y)\n",
    "\n",
    "        results['raw'] = result\n",
    "        results['m_fit'] = m_fit\n",
    "        if cal:\n",
    "            print \"calibrated:\"\n",
    "            m_c = CalibratedClassifierCV(m_fit, method = cal_m, cv='prefit')\n",
    "            m_fit_c = m_c.fit(cal_x, cal_y)\n",
    "            result_c = self.make_result(m_fit_c, self.data.test_x[features], self.data.test_y)\n",
    "            results['calibrated'] = result_c              \n",
    "            print \"\\n\"\n",
    "        if name in self.rundict:\n",
    "            self.rundict[name].update(results)\n",
    "        else:\n",
    "            self.rundict.update({name:results})\n",
    "    \n",
    "    #Run from tuned set\n",
    "    def run_tuned(self, name, cal=True, cal_m='sigmoid'):\n",
    "        self.run_model(name, self.rundict[name]['model'], self.rundict[name]['features'], cal, cal_m)\n",
    "    \n",
    "    #Output rundict to csv\n",
    "    def to_csv(self):\n",
    "        if self.rundict == {}:\n",
    "            raise ValueError('No results found')\n",
    "        else:\n",
    "            now = pd.to_datetime('today').value\n",
    "            #Make dataframe, transpose so each row = model\n",
    "            pd.DataFrame(self.rundict).T.to_csv('results_{}.csv'.format(now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "The approach here is to create 3 time-lag features:\n",
    "\n",
    "1. crashes in the past week\n",
    "2. crashes in the past month\n",
    "3. crashes in the past quarter (three months)\n",
    "4. average crashes per week up to target week\n",
    "\n",
    "All features except 4 are calculated to exclude one another.  That is, crashes in the past month does not include the past week's crashes.  Crashes in the past quarter do not include the past month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEG_CHARS = ['AADT', 'SPEEDLIMIT', 'Struct_Cnd', 'Surface_Tp', 'F_F_Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data = pd.read_csv('../../data/processed/vz_predict_dataset.csv.gz', compression='gzip', dtype={'segment_id':'str'})\n",
    "data.sort_values(['segment_id', 'week'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get segments with non-zero crashes\n",
    "data_nonzero = data.set_index('segment_id').loc[data.groupby('segment_id').crash.sum()>0]\n",
    "data_nonzero.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_crash_data(data, col, target):\n",
    "    \"\"\" formats crash data for train/test \n",
    "    target: week to predict (make into binary target)\n",
    "        must be >4 months in\n",
    "    gets previous week count, previous month count, previous quarter count, avg per week\n",
    "    \n",
    "    \"\"\"\n",
    "    assert target>16\n",
    "    pre_week = target - 1\n",
    "    pre_month = range(pre_week-4, target)\n",
    "    pre_quarter = range(pre_month[0]-12, target)\n",
    "    all_prior_weeks = range(1, target)\n",
    "    \n",
    "    # week interval for each segment\n",
    "    # full range = pre_quarter : target\n",
    "    sliced = data.loc[(slice(None),slice(1, target)),:]\n",
    "    week_data = sliced[col].unstack(1)\n",
    "    week_data.reset_index(level=1, inplace=True)\n",
    "    \n",
    "    # aggregate\n",
    "    week_data['pre_month'] = week_data[pre_month].sum(axis=1)\n",
    "    week_data['pre_quarter'] = week_data[pre_quarter].sum(axis=1)\n",
    "    week_data['pre_week'] = week_data[pre_week]\n",
    "    # avg as of target week\n",
    "    week_data['avg_week'] = week_data[all_prior_weeks].apply(\n",
    "        lambda x: x.sum() / len(all_prior_weeks), axis=1\n",
    "    )\n",
    "    \n",
    "    # binarize target\n",
    "    week_data['target'] = (week_data[target]>0).astype(int)\n",
    "    \n",
    "    return(week_data[['segment_id','target', 'pre_week', \n",
    "                      'pre_month', 'pre_quarter', 'avg_week']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# arbitrarily choosing week = 50\n",
    "crash_lags = format_crash_data(data_nonzero.set_index(['segment_id','week']), 'crash', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_segs = data_nonzero.groupby('segment_id')[SEG_CHARS].max()  # grab the highest values from each column for a segment, not used in model?\n",
    "data_segs.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_model = crash_lags.merge(data_segs, left_on='segment_id', right_on='segment_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in adjacency info\n",
    "adj = pd.read_csv('../../data/processed/adjacency_info.csv', dtype={'segment_id':'str'})\n",
    "# only include adj that are in data model\n",
    "adj = adj[adj.segment_id.isin(data_model.segment_id)]\n",
    "# create adj matrix (1 if is adjacent, 0 otherwise)\n",
    "adj_mat = adj.merge(adj, on='orig_id')\n",
    "adj_mat = adj_mat.drop(['orig_id'], axis=1)\n",
    "adj_mat = pd.concat([adj_mat.segment_id_x, pd.get_dummies(adj_mat.segment_id_y)], axis=1)\n",
    "adj_mat = adj_mat.groupby('segment_id_x').max()\n",
    "adj_mat = adj_mat.apply(lambda x: x.astype(float))\n",
    "# fill diagonal (self) with zero\n",
    "np.fill_diagonal(adj_mat.values, 0)\n",
    "adj_mat.index.name = 'segment_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     segment_id_x segment_id_y segment_id  target  pre_week  pre_month  \\\n",
      "2227      0010194         4686       4686       0       0.0        1.0   \n",
      "2232      0010194         4870       4870       0       0.0        0.0   \n",
      "661       0010205         4958       4958       0       0.0        0.0   \n",
      "623       0010225         4418       4418       0       0.0        0.0   \n",
      "904       0010257         1559       1559       1       0.0        0.0   \n",
      "1940       001026         2692       2692       0       0.0        0.0   \n",
      "1822      0010306         1982       1982       0       0.0        0.0   \n",
      "1019      0010315         1937       1937       0       0.0        0.0   \n",
      "2398      0010319         4306       4306       0       0.0        1.0   \n",
      "524       0010334         3421       3421       0       0.0        0.0   \n",
      "306       0010362         1729       1729       0       0.0        0.0   \n",
      "504       0010362         1994       1994       0       0.0        0.0   \n",
      "2355      0010371         1278       1278       0       0.0        0.0   \n",
      "664       0010641         4852       4852       0       0.0        0.0   \n",
      "150       0010692         3537       3537       0       0.0        0.0   \n",
      "2042      0010710         3654       3654       0       0.0        0.0   \n",
      "1523      0010825         4159       4159       0       0.0        0.0   \n",
      "548       0010855         3917       3917       0       0.0        0.0   \n",
      "2123      0010914         4467       4467       0       0.0        0.0   \n",
      "2110      0010914         4533       4533       0       0.0        0.0   \n",
      "2330      0010962         2435       2435       0       0.0        0.0   \n",
      "293         00110          818        818       0       1.0        1.0   \n",
      "1525      0011050         4141       4141       0       0.0        1.0   \n",
      "505       0011066         1994       1994       0       0.0        0.0   \n",
      "1959      0011096          386        386       0       0.0        0.0   \n",
      "1220      0011119         6943       6943       0       0.0        0.0   \n",
      "205       0011123         6905       6905       0       0.0        0.0   \n",
      "378       0011128         8502       8502       0       0.0        0.0   \n",
      "1492      0011154         8419       8419       0       0.0        0.0   \n",
      "2179      0011181         8127       8127       0       1.0        2.0   \n",
      "...           ...          ...        ...     ...       ...        ...   \n",
      "3089          NaN          NaN       7800       0       0.0        0.0   \n",
      "3090          NaN          NaN        781       0       0.0        0.0   \n",
      "3091          NaN          NaN       7833       0       0.0        0.0   \n",
      "3092          NaN          NaN       7954       0       0.0        1.0   \n",
      "3093          NaN          NaN       7999       0       0.0        0.0   \n",
      "3094          NaN          NaN       8028       0       0.0        0.0   \n",
      "3095          NaN          NaN        809       0       0.0        0.0   \n",
      "3096          NaN          NaN       8096       0       0.0        0.0   \n",
      "3097          NaN          NaN       8104       0       0.0        0.0   \n",
      "3098          NaN          NaN       8213       0       0.0        1.0   \n",
      "3099          NaN          NaN       8218       0       0.0        0.0   \n",
      "3100          NaN          NaN       8237       0       0.0        0.0   \n",
      "3101          NaN          NaN        826       0       0.0        0.0   \n",
      "3102          NaN          NaN       8288       0       0.0        1.0   \n",
      "3103          NaN          NaN       8296       0       0.0        0.0   \n",
      "3104          NaN          NaN       8345       0       0.0        0.0   \n",
      "3105          NaN          NaN       8363       0       0.0        0.0   \n",
      "3106          NaN          NaN       8378       0       0.0        0.0   \n",
      "3107          NaN          NaN       8396       0       0.0        0.0   \n",
      "3108          NaN          NaN       8427       0       0.0        0.0   \n",
      "3109          NaN          NaN       8444       0       0.0        0.0   \n",
      "3110          NaN          NaN       8457       1       0.0        0.0   \n",
      "3111          NaN          NaN       8504       0       0.0        0.0   \n",
      "3112          NaN          NaN       8516       0       0.0        0.0   \n",
      "3113          NaN          NaN       8534       0       0.0        0.0   \n",
      "3114          NaN          NaN       8554       0       1.0        1.0   \n",
      "3115          NaN          NaN         86       0       0.0        0.0   \n",
      "3116          NaN          NaN        931       0       0.0        0.0   \n",
      "3117          NaN          NaN        937       0       0.0        0.0   \n",
      "3118          NaN          NaN        969       0       0.0        0.0   \n",
      "\n",
      "      pre_quarter  avg_week  \n",
      "2227          3.0  0.102041  \n",
      "2232          0.0  0.020408  \n",
      "661           2.0  0.142857  \n",
      "623           3.0  0.122449  \n",
      "904           1.0  0.040816  \n",
      "1940          0.0  0.020408  \n",
      "1822          1.0  0.040816  \n",
      "1019          0.0  0.020408  \n",
      "2398          2.0  0.040816  \n",
      "524           0.0  0.020408  \n",
      "306           0.0  0.020408  \n",
      "504           1.0  0.020408  \n",
      "2355          0.0  0.081633  \n",
      "664           1.0  0.020408  \n",
      "150           1.0  0.020408  \n",
      "2042          0.0  0.040816  \n",
      "1523          0.0  0.020408  \n",
      "548           0.0  0.020408  \n",
      "2123          0.0  0.020408  \n",
      "2110          0.0  0.020408  \n",
      "2330          0.0  0.020408  \n",
      "293           2.0  0.040816  \n",
      "1525          4.0  0.224490  \n",
      "505           1.0  0.020408  \n",
      "1959          1.0  0.020408  \n",
      "1220          1.0  0.040816  \n",
      "205           0.0  0.081633  \n",
      "378           1.0  0.040816  \n",
      "1492          1.0  0.122449  \n",
      "2179          5.0  0.142857  \n",
      "...           ...       ...  \n",
      "3089          0.0  0.020408  \n",
      "3090          0.0  0.040816  \n",
      "3091          4.0  0.122449  \n",
      "3092          1.0  0.020408  \n",
      "3093          0.0  0.020408  \n",
      "3094          1.0  0.020408  \n",
      "3095          0.0  0.020408  \n",
      "3096          0.0  0.020408  \n",
      "3097          1.0  0.040816  \n",
      "3098          2.0  0.163265  \n",
      "3099          0.0  0.020408  \n",
      "3100          0.0  0.020408  \n",
      "3101          0.0  0.020408  \n",
      "3102          1.0  0.081633  \n",
      "3103          1.0  0.040816  \n",
      "3104          1.0  0.020408  \n",
      "3105          0.0  0.020408  \n",
      "3106          0.0  0.020408  \n",
      "3107          2.0  0.040816  \n",
      "3108          0.0  0.040816  \n",
      "3109          1.0  0.020408  \n",
      "3110          0.0  0.000000  \n",
      "3111          1.0  0.040816  \n",
      "3112          0.0  0.020408  \n",
      "3113          0.0  0.020408  \n",
      "3114          1.0  0.040816  \n",
      "3115          0.0  0.020408  \n",
      "3116          0.0  0.020408  \n",
      "3117          0.0  0.020408  \n",
      "3118          0.0  0.020408  \n",
      "\n",
      "[3119 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add in adjacency info\n",
    "adj_info = pd.read_csv('../../data/processed/adjacency_info.csv', usecols=['segment_id', 'orig_id'],\n",
    "                       dtype={'segment_id':'str', 'orig_id':'str'})\n",
    "\n",
    "# link adjacent segments for segments with crashes\n",
    "adj_info = adj_info[adj_info.segment_id.isin(data_model.segment_id)]\n",
    "adj_mat = adj_info.merge(adj_info, on='orig_id')\n",
    "adj_mat = adj_mat[['segment_id_x', 'segment_id_y']]\n",
    "adj_mat.drop_duplicates(inplace=True)\n",
    "adj_mat = adj_mat[adj_mat.segment_id_x != adj_mat.segment_id_y]\n",
    "\n",
    "def get_adj_crash_lags(target_week):\n",
    "    \"\"\"calculate total number of crashes that occurred \n",
    "    in adjacent segments for target week and lags as defined in format_crash_data\n",
    "    \"\"\" \n",
    "    lag_data = format_crash_data(data_nonzero.set_index(['segment_id','week']), 'crash', target_week)\n",
    "    merge_lags = adj_mat.merge(lag_data, left_on='segment_id_y', right_on='segment_id', how='right')\n",
    "    print merge_lags.sort_values('segment_id_x')\n",
    "    adj_lags = merge_lags.groupby(['segment_id_x'])['target', 'pre_week', 'pre_month', 'pre_quarter'].sum()\n",
    "    return adj_lags\n",
    "\n",
    "adj_lags = get_adj_crash_lags(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id_x</th>\n",
       "      <th>segment_id_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6797</td>\n",
       "      <td>7068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7068</td>\n",
       "      <td>6797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6797</td>\n",
       "      <td>6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6701</td>\n",
       "      <td>6797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>271</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>265</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>276</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>404</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2143</td>\n",
       "      <td>2062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2062</td>\n",
       "      <td>2143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2143</td>\n",
       "      <td>006567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>006567</td>\n",
       "      <td>2143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2143</td>\n",
       "      <td>0014575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0014575</td>\n",
       "      <td>2143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2681</td>\n",
       "      <td>00118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>00118</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>93</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>871</td>\n",
       "      <td>008304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>008304</td>\n",
       "      <td>871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>871</td>\n",
       "      <td>829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>829</td>\n",
       "      <td>871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>8012</td>\n",
       "      <td>8072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>8072</td>\n",
       "      <td>8012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>392</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>346</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>391</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>335</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2305</td>\n",
       "      <td>2294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2294</td>\n",
       "      <td>2305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9784</th>\n",
       "      <td>3168</td>\n",
       "      <td>3164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9785</th>\n",
       "      <td>3164</td>\n",
       "      <td>3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9793</th>\n",
       "      <td>3169</td>\n",
       "      <td>00786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9794</th>\n",
       "      <td>00786</td>\n",
       "      <td>3169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9797</th>\n",
       "      <td>3169</td>\n",
       "      <td>002812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>002812</td>\n",
       "      <td>3169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9810</th>\n",
       "      <td>731</td>\n",
       "      <td>008392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9811</th>\n",
       "      <td>008392</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9819</th>\n",
       "      <td>6631</td>\n",
       "      <td>002491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9820</th>\n",
       "      <td>002491</td>\n",
       "      <td>6631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9842</th>\n",
       "      <td>2009</td>\n",
       "      <td>0012080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9843</th>\n",
       "      <td>0012080</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9846</th>\n",
       "      <td>2009</td>\n",
       "      <td>0014042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9847</th>\n",
       "      <td>0014042</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9850</th>\n",
       "      <td>2009</td>\n",
       "      <td>0014043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9851</th>\n",
       "      <td>0014043</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>7277</td>\n",
       "      <td>0016654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9886</th>\n",
       "      <td>0016654</td>\n",
       "      <td>7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>6834</td>\n",
       "      <td>0016997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>0016997</td>\n",
       "      <td>6834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9904</th>\n",
       "      <td>6839</td>\n",
       "      <td>0014728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>0014728</td>\n",
       "      <td>6839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>7646</td>\n",
       "      <td>007938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>007938</td>\n",
       "      <td>7646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9934</th>\n",
       "      <td>5639</td>\n",
       "      <td>0013617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9935</th>\n",
       "      <td>0013617</td>\n",
       "      <td>5639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9938</th>\n",
       "      <td>5639</td>\n",
       "      <td>0013620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9939</th>\n",
       "      <td>0013620</td>\n",
       "      <td>5639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9955</th>\n",
       "      <td>8485</td>\n",
       "      <td>007607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9956</th>\n",
       "      <td>007607</td>\n",
       "      <td>8485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2452 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     segment_id_x segment_id_y\n",
       "7            6797         7068\n",
       "8            7068         6797\n",
       "11           6797         6701\n",
       "12           6701         6797\n",
       "21            271          265\n",
       "22            265          271\n",
       "30            276          404\n",
       "31            404          276\n",
       "35           2143         2062\n",
       "36           2062         2143\n",
       "39           2143       006567\n",
       "40         006567         2143\n",
       "43           2143      0014575\n",
       "44        0014575         2143\n",
       "51           2681        00118\n",
       "52          00118         2681\n",
       "65             93           88\n",
       "66             88           93\n",
       "70            871       008304\n",
       "71         008304          871\n",
       "74            871          829\n",
       "75            829          871\n",
       "82           8012         8072\n",
       "83           8072         8012\n",
       "95            392          346\n",
       "96            346          392\n",
       "103           391          335\n",
       "104           335          391\n",
       "109          2305         2294\n",
       "110          2294         2305\n",
       "...           ...          ...\n",
       "9784         3168         3164\n",
       "9785         3164         3168\n",
       "9793         3169        00786\n",
       "9794        00786         3169\n",
       "9797         3169       002812\n",
       "9798       002812         3169\n",
       "9810          731       008392\n",
       "9811       008392          731\n",
       "9819         6631       002491\n",
       "9820       002491         6631\n",
       "9842         2009      0012080\n",
       "9843      0012080         2009\n",
       "9846         2009      0014042\n",
       "9847      0014042         2009\n",
       "9850         2009      0014043\n",
       "9851      0014043         2009\n",
       "9885         7277      0016654\n",
       "9886      0016654         7277\n",
       "9895         6834      0016997\n",
       "9896      0016997         6834\n",
       "9904         6839      0014728\n",
       "9905      0014728         6839\n",
       "9909         7646       007938\n",
       "9910       007938         7646\n",
       "9934         5639      0013617\n",
       "9935      0013617         5639\n",
       "9938         5639      0013620\n",
       "9939      0013620         5639\n",
       "9955         8485       007607\n",
       "9956       007607         8485\n",
       "\n",
       "[2452 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>target</th>\n",
       "      <th>pre_week</th>\n",
       "      <th>pre_month</th>\n",
       "      <th>pre_quarter</th>\n",
       "      <th>avg_week</th>\n",
       "      <th>AADT</th>\n",
       "      <th>SPEEDLIMIT</th>\n",
       "      <th>Struct_Cnd</th>\n",
       "      <th>Surface_Tp</th>\n",
       "      <th>F_F_Class</th>\n",
       "      <th>target_adj</th>\n",
       "      <th>pre_week_adj</th>\n",
       "      <th>pre_month_adj</th>\n",
       "      <th>pre_quarter_adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0010004</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0010194</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>9909</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0010205</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>9850</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0010225</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>197004</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  segment_id  target  pre_week  pre_month  pre_quarter  avg_week    AADT  \\\n",
       "0       0010       0       0.0        0.0          0.0  0.020408       0   \n",
       "1    0010004       0       0.0        0.0          0.0  0.020408       0   \n",
       "2    0010194       0       1.0        1.0          1.0  0.020408    9909   \n",
       "3    0010205       0       0.0        0.0          1.0  0.020408    9850   \n",
       "4    0010225       0       0.0        0.0          1.0  0.061224  197004   \n",
       "\n",
       "   SPEEDLIMIT  Struct_Cnd  Surface_Tp  F_F_Class  target_adj  pre_week_adj  \\\n",
       "0          20           2           6          7           0           0.0   \n",
       "1          20           1           6          7           0           0.0   \n",
       "2          30           2           6          4           0           1.0   \n",
       "3          25           2           6          4           0           0.0   \n",
       "4          65           2           6          1           0           0.0   \n",
       "\n",
       "   pre_month_adj  pre_quarter_adj  \n",
       "0            0.0              0.0  \n",
       "1            0.0              0.0  \n",
       "2            1.0              1.0  \n",
       "3            0.0              1.0  \n",
       "4            0.0              1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model = data_model.merge(adj_lags, how='left', left_on='segment_id', right_index=True, suffixes=('', '_adj'))\n",
    "data_model.fillna(0, inplace=True)\n",
    "\n",
    "data_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning\n",
    "This uses the model helpers above.  They're based on sklearn and implement a randomized grid search with K-fold crossvalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train obs: 1370\n",
      "Test obs: 608\n"
     ]
    }
   ],
   "source": [
    "#Initialize data\n",
    "#Fill missing 0 (Only affects tot_crash)\n",
    "df = Indata(data_model, 'target')\n",
    "#Create train/test split\n",
    "df.tr_te_split(.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters for model\n",
    "#Model parameters\n",
    "params = dict()\n",
    "\n",
    "#cv parameters\n",
    "cvp = dict()\n",
    "cvp['pmetric'] = 'roc_auc'\n",
    "cvp['iter'] = 5 #number of iterations\n",
    "cvp['folds'] = 5 #folds for cv (default)\n",
    "\n",
    "#LR parameters\n",
    "mp = dict()\n",
    "mp['LogisticRegression'] = dict()\n",
    "mp['LogisticRegression']['penalty'] = ['l1','l2']\n",
    "mp['LogisticRegression']['C'] = ss.beta(a=5,b=2) #beta distribution for selecting reg strength\n",
    "\n",
    "#RF model parameters\n",
    "mp['RandomForestClassifier'] = dict()\n",
    "mp['RandomForestClassifier']['n_estimators'] = [2**8] #number of trees in the forest\n",
    "mp['RandomForestClassifier']['max_features'] = ss.beta(a=5,b=2) #number of features at split\n",
    "mp['RandomForestClassifier']['max_leaf_nodes'] = ss.nbinom(n=2,p=0.001,loc=100) #max number of leaves to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features\n",
    "features = [u'pre_week', u'pre_month', u'pre_quarter', 'avg_week', u'AADT', u'SPEEDLIMIT',\n",
    "            u'Struct_Cnd', u'Surface_Tp', u'F_F_Class', u'target_adj', u'pre_week_adj', \n",
    "            u'pre_month_adj', u'pre_quarter_adj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize tuner\n",
    "tune = Tuner(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   24.3s finished\n"
     ]
    }
   ],
   "source": [
    "#Base RF model\n",
    "tune.tune('RF_base', 'RandomForestClassifier', features, cvp, mp['RandomForestClassifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "#Base LR model\n",
    "tune.tune('LR_base', 'LogisticRegression', features, cvp, mp['LogisticRegression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>params</th>\n",
       "      <th>name</th>\n",
       "      <th>m_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{u'max_features': 0.92761218686, u'max_leaf_no...</td>\n",
       "      <td>RF_base</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{u'max_features': 0.92250981978, u'max_leaf_no...</td>\n",
       "      <td>RF_base</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{u'max_features': 0.717020790816, u'max_leaf_n...</td>\n",
       "      <td>RF_base</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{u'max_features': 0.390297103556, u'max_leaf_n...</td>\n",
       "      <td>RF_base</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{u'max_features': 0.331545646012, u'max_leaf_n...</td>\n",
       "      <td>RF_base</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{u'penalty': u'l1', u'C': 0.59134947425}</td>\n",
       "      <td>LR_base</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{u'penalty': u'l2', u'C': 0.825827911429}</td>\n",
       "      <td>LR_base</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{u'penalty': u'l2', u'C': 0.749919388514}</td>\n",
       "      <td>LR_base</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{u'penalty': u'l1', u'C': 0.513137560622}</td>\n",
       "      <td>LR_base</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{u'penalty': u'l2', u'C': 0.514129917419}</td>\n",
       "      <td>LR_base</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  mean_train_score  \\\n",
       "0              1.0               1.0   \n",
       "1              1.0               1.0   \n",
       "2              1.0               1.0   \n",
       "3              1.0               1.0   \n",
       "4              1.0               1.0   \n",
       "0              1.0               1.0   \n",
       "1              1.0               1.0   \n",
       "2              1.0               1.0   \n",
       "3              1.0               1.0   \n",
       "4              1.0               1.0   \n",
       "\n",
       "                                              params     name  \\\n",
       "0  {u'max_features': 0.92761218686, u'max_leaf_no...  RF_base   \n",
       "1  {u'max_features': 0.92250981978, u'max_leaf_no...  RF_base   \n",
       "2  {u'max_features': 0.717020790816, u'max_leaf_n...  RF_base   \n",
       "3  {u'max_features': 0.390297103556, u'max_leaf_n...  RF_base   \n",
       "4  {u'max_features': 0.331545646012, u'max_leaf_n...  RF_base   \n",
       "0           {u'penalty': u'l1', u'C': 0.59134947425}  LR_base   \n",
       "1          {u'penalty': u'l2', u'C': 0.825827911429}  LR_base   \n",
       "2          {u'penalty': u'l2', u'C': 0.749919388514}  LR_base   \n",
       "3          {u'penalty': u'l1', u'C': 0.513137560622}  LR_base   \n",
       "4          {u'penalty': u'l2', u'C': 0.514129917419}  LR_base   \n",
       "\n",
       "                   m_name  \n",
       "0  RandomForestClassifier  \n",
       "1  RandomForestClassifier  \n",
       "2  RandomForestClassifier  \n",
       "3  RandomForestClassifier  \n",
       "4  RandomForestClassifier  \n",
       "0      LogisticRegression  \n",
       "1      LogisticRegression  \n",
       "2      LogisticRegression  \n",
       "3      LogisticRegression  \n",
       "4      LogisticRegression  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display results\n",
    "tune.grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting RF_base model with 13 features\n",
      "f1_score:  1.0\n",
      "brier_score:  0.0\n",
      "AUC score:  1.0\n",
      "Confusion Matrix:  [[592   0]\n",
      " [  0  16]]\n"
     ]
    }
   ],
   "source": [
    "# Run test\n",
    "test = Tester(df)\n",
    "test.init_tuned(tune)\n",
    "test.run_tuned('RF_base', cal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'target_adj', 0.98588179121917696), ('avg_week', 0.013727160469520652), (u'F_F_Class', 0.00011328668923114565), (u'AADT', 0.00010752681903806563), (u'Struct_Cnd', 8.7994059457898347e-05), (u'pre_week', 6.3251258746217833e-05), (u'SPEEDLIMIT', 1.8989484828932132e-05), (u'pre_month', 0.0), (u'pre_quarter', 0.0), (u'Surface_Tp', 0.0), (u'pre_week_adj', 0.0), (u'pre_month_adj', 0.0), (u'pre_quarter_adj', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# Check feature importance\n",
    "f_importance = tune.best_models['RF_base']['m_fit'].feature_importances_\n",
    "fi = list(zip(features, f_importance))\n",
    "print sorted(fi, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift chart by \"risk bin\"\n",
    "The classifier problem is difficult because the classes are unbalanced (.05% have crashes at target week).  More useful are the probabilities being produced by the model, which give some idea of risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lift_chart(x_col, y_col, data, ax=None):\n",
    "\n",
    "    p = sns.barplot(x=x_col, y=y_col, data=data, \n",
    "                    palette='Reds', ax = None, ci=None)\n",
    "    vals = p.get_yticks()\n",
    "    p.set_yticklabels(['{:3.0f}%'.format(i*100) for i in vals])\n",
    "    xvals = [x.get_text().split(',')[-1].strip(']') for x in p.get_xticklabels()]\n",
    "    xvals = ['{:3.0f}%'.format(float(x)*100) for x in xvals]\n",
    "    p.set_xticklabels(xvals)\n",
    "    p.set_facecolor('white')\n",
    "    p.set_xlabel('')\n",
    "    p.set_ylabel('')\n",
    "    p.set_title('Predicted probability vs actual percent')\n",
    "    return(p)\n",
    "    \n",
    "def density(data, score, ax=None):\n",
    "    p = sns.kdeplot(risk_df['risk_score'], ax=ax)\n",
    "    p.set_facecolor('white')\n",
    "    p.legend('')\n",
    "    p.set_xlabel('Predicted probability of crash')\n",
    "    p.set_title('KDE plot predictions')\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    608.000000\n",
      "mean       0.026316\n",
      "std        0.160204\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        0.000000\n",
      "max        1.000000\n",
      "Name: risk_score, dtype: float64\n",
      "categories\n",
      "(-1, 0]         592\n",
      "(0, 0.01]         0\n",
      "(0.01, 0.02]      0\n",
      "(0.02, 0.05]      0\n",
      "(0.05, 1]        16\n",
      "Name: crash, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "risk_scores = test.rundict['RF_base']['m_fit'].predict_proba(test.data.test_x[features])[:,1]\n",
    "risk_df = pd.DataFrame({'risk_score':risk_scores, 'crash':test.data.test_y})\n",
    "print risk_df.risk_score.describe()\n",
    "risk_df['categories'] = pd.cut(risk_df['risk_score'], bins=[-1, 0, .01, .02, .05, max(risk_scores)])\n",
    "risk_mean = risk_df.groupby('categories')['crash'].count()\n",
    "print risk_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11bbccbd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAFlCAYAAACnT5IMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f0ab490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "lift_chart('categories', 'crash', risk_df, \n",
    "           ax=axes[1])\n",
    "density(risk_df, 'risk_score', ax=axes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output predictions\n",
    "# predict on all segments\n",
    "data_model['risk_score'] = test.rundict['RF_base']['m_fit'].predict_proba(data_model[features])[:,1]\n",
    "data_model.to_csv('seg_with_risk_score_adj.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check sensitivity to week\n",
    "I predicted an arbitrary week as target here, but I'd like to see whether things change significantly if I change that week.  A good metric to measure that is brier score loss.  It'll be low throughout as the classifier doesn't perform great, but it shouldn't vary a huge amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week  20\n",
      "Train obs: 931\n",
      "Test obs: 380\n",
      "Fitting RF_base model with 13 features\n",
      "f1_score:  0.0\n",
      "brier_score:  0.037701182775\n",
      "AUC score:  0.497267759563\n",
      "Confusion Matrix:  [[364   2]\n",
      " [ 14   0]]\n",
      "\n",
      "\n",
      "week  30\n",
      "Train obs: 896\n",
      "Test obs: 415\n",
      "Fitting RF_base model with 13 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alice\\Anaconda2\\envs\\boston-crash-model\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.0\n",
      "brier_score:  0.046533491401\n",
      "AUC score:  0.5\n",
      "Confusion Matrix:  [[396   0]\n",
      " [ 19   0]]\n",
      "\n",
      "\n",
      "week  40\n",
      "Train obs: 941\n",
      "Test obs: 370\n",
      "Fitting RF_base model with 13 features\n",
      "f1_score:  0.0\n",
      "brier_score:  0.0615806052203\n",
      "AUC score:  0.489971346705\n",
      "Confusion Matrix:  [[342   7]\n",
      " [ 21   0]]\n",
      "\n",
      "\n",
      "week  50\n",
      "Train obs: 896\n",
      "Test obs: 415\n",
      "Fitting RF_base model with 13 features\n",
      "f1_score:  0.0\n",
      "brier_score:  0.0348137545298\n",
      "AUC score:  0.4975\n",
      "Confusion Matrix:  [[398   2]\n",
      " [ 15   0]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in [20, 30, 40, 50]:\n",
    "    print \"week \", w\n",
    "    crash_lags = format_crash_data(data_nonzero.set_index(['segment_id','week']), 'crash', w)\n",
    "    data_model = crash_lags.merge(data_segs, left_on='segment_id', right_on='segment_id')\n",
    "    adj_lags = get_adj_crash_lags(w)\n",
    "    data_model = data_model.merge(adj_lags, left_on='segment_id', right_index=True, suffixes=('', '_adj'))\n",
    "    df = Indata(data_model, 'target')\n",
    "    # create train/test split\n",
    "    df.tr_te_split(.7)\n",
    "    test = Tester(df)\n",
    "    test.init_tuned(tune)\n",
    "    test.run_tuned('RF_base', cal=False)\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
